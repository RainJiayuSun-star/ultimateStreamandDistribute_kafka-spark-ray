FROM ubuntu:22.04

# Install required packages
RUN apt-get update && apt-get install -y \
    wget \
    curl \
    openjdk-11-jdk \
    python3 \
    python3-pip \
    net-tools \
    lsof \
    nano \
    netcat-openbsd \
    && rm -rf /var/lib/apt/lists/*

# Set Java environment
ENV JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
ENV PATH="${PATH}:${JAVA_HOME}/bin"

# Download and extract Spark
# Using Spark 3.5.0 (stable version available on archive.apache.org)
RUN wget https://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz && \
    tar -xf spark-3.5.0-bin-hadoop3.tgz && \
    rm spark-3.5.0-bin-hadoop3.tgz

# Set Spark environment variables
ENV SPARK_HOME=/spark-3.5.0-bin-hadoop3
ENV PATH="${PATH}:${SPARK_HOME}/bin:${SPARK_HOME}/sbin"

# Install Python dependencies for Spark streaming
RUN pip3 install --no-cache-dir \
    kafka-python==2.0.2 \
    pyspark==3.5.0

# Create directory for application code
WORKDIR /app

# Copy Spark application code
# Copy to both /app/spark/ (for imports) and /app/src/spark/ (for spark-submit path)
COPY src/spark/ /app/spark/
COPY src/spark/ /app/src/spark/
COPY src/utils/ /app/utils/
COPY src/utils/ /app/src/utils/
COPY src/kafka_weather/ /app/kafka_weather/
COPY src/kafka_weather/ /app/src/kafka_weather/
COPY scripts/start_spark_streaming.sh /app/start_spark_streaming.sh
RUN chmod +x /app/start_spark_streaming.sh

# Set Python path
ENV PYTHONPATH=/app:$PYTHONPATH

# Expose Spark ports
EXPOSE 8080 7077

# Default command (can be overridden in docker-compose)
CMD ["spark-class", "org.apache.spark.deploy.master.Master"]

