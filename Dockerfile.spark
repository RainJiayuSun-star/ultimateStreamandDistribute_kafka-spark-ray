FROM ubuntu:22.04

# Install required packages
RUN apt-get update && apt-get install -y \
    wget \
    curl \
    openjdk-11-jdk \
    python3 \
    python3-pip \
    net-tools \
    lsof \
    nano \
    && rm -rf /var/lib/apt/lists/*

# Set Java environment
ENV JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
ENV PATH="${PATH}:${JAVA_HOME}/bin"

# Download and extract Spark
RUN wget https://dlcdn.apache.org/spark/spark-3.5.7/spark-3.5.7-bin-hadoop3.tgz && \
    tar -xf spark-3.5.7-bin-hadoop3.tgz && \
    rm spark-3.5.7-bin-hadoop3.tgz

# Set Spark environment variables
ENV SPARK_HOME=/spark-3.5.7-bin-hadoop3
ENV PATH="${PATH}:${SPARK_HOME}/bin:${SPARK_HOME}/sbin"

# Install Python dependencies for Spark streaming
RUN pip3 install --no-cache-dir \
    kafka-python==2.0.2 \
    pyspark==3.5.7

# Create directory for application code
WORKDIR /app

# Copy Spark application code
COPY src/spark/ /app/spark/
COPY src/utils/ /app/utils/
COPY src/kafka/ /app/kafka/

# Set Python path
ENV PYTHONPATH=/app:$PYTHONPATH

# Expose Spark ports
EXPOSE 8080 7077

# Default command (can be overridden in docker-compose)
CMD ["spark-class", "org.apache.spark.deploy.master.Master"]

